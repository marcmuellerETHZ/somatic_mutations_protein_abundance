{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281e1b78",
   "metadata": {},
   "source": [
    "# DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9b7d0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import scipy as sp\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import gc\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools import add_constant\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from Bio import SeqIO\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from adjustText import adjust_text\n",
    "\n",
    "#Style\n",
    "sns.set(style=\"white\")\n",
    "sns.set_palette('Paired')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a3cf44",
   "metadata": {},
   "source": [
    "# Data file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da0e044",
   "metadata": {},
   "source": [
    "file_dat_res from https://github.com/jurgjn/af2genomics/blob/main/notebooks/interaction_interfaces/somatic_mutations_protein_abundance_ifresid.tsv \\\n",
    "(accession 24.05.2024)\n",
    "\n",
    "filte_dat_vep from `/cluster/work/beltrao/jjaenes/23.11.09_cptac_missense_mutations/cptac_missense_mutations_merge_23.11.1.tsv` \\\n",
    "(accession 10.11.2023)\n",
    "\n",
    "file_biogrid_all from [BioGRID](https://downloads.thebiogrid.org/BioGRID)\\\n",
    "(accession 25.05.2023)\\\n",
    "Version 4.4.227\\\n",
    "File: BIOGRID-ALL-4.4.227.tab3.zip\n",
    "\n",
    "file_biogrid_mv from [BioGRID](https://downloads.thebiogrid.org/BioGRID)\\\n",
    "(accession 26.05.2023)\\\n",
    "Version 4.4.227\\\n",
    "File: BIOGRID-MV-Physical-4.4.227.tab3.zip\n",
    "\n",
    "file_string_all from [STRING](https://string-db.org/cgi/download?sessionId=bJrNsFgGvKiS&species_text=Homo+sapiens)\\\n",
    "(accession 01.11.2023)\\\n",
    "Version 12.0\\\n",
    "File: 9606.protein.links.v12.0.txt.gz\n",
    "\n",
    "file_string_physical from [STRING](https://string-db.org/cgi/download?sessionId=bJrNsFgGvKiS&species_text=Homo+sapiens)\\\n",
    "(accession 01.11.2023)\\\n",
    "Version 12.0\\\n",
    "File: 9606.protein.physical.links.detailed.v12.0.txt.gz\n",
    "\n",
    "file_translation_hgnc_update from [HGNC BioMart](https://biomart.genenames.org/martform/#!/default/HGNC?datasets=hgnc_gene_mart) (accession 31.10.2023) \\\n",
    "Locus group: protein-coding gene \\\n",
    "_Approved symbol, Alias symbol, Previous symbol_\n",
    "\n",
    "file_translation_hgnc_uniprot from [HGNC BioMart](https://biomart.genenames.org/martform/#!/default/HGNC?datasets=hgnc_gene_mart) (accession 31.10.2023) \\\n",
    "Locus group: protein-coding gene \\\n",
    "_Approved symbol, UniProt accession_\n",
    "\n",
    "file_translation_hgnc_ensembl from [ensembl BioMart](https://www.ensembl.org/biomart/martview/834e6d51cb98b100507df185b4d6cadf) (accession 31.10.2023) \\\n",
    "Dataset: GRCh38.p14, Filters: Limit to genes with HGNC Symbol ID(s) \\\n",
    "_Protein stable ID, Gene name_\n",
    "\n",
    "directory_fasta from David Burke, 10.07.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eebb41",
   "metadata": {},
   "source": [
    "# File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "218489f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# omics data\n",
    "file_dat_annot = \"data/all_samples_annotation.txt\"\n",
    "file_dat_cnv = \"data/cnv.txt\"\n",
    "file_dat_tx = \"data/transcriptomics_log2fc.txt\"\n",
    "file_dat_px = \"data/proteomics.txt\"\n",
    "file_dat_mt = \"data/mutations.txt\"\n",
    "\n",
    "# predicted interface data\n",
    "#file_dat_res = \"data/interface_full_strict.txt\"\n",
    "file_dat_res = \"data/somatic_mutations_protein_abundance_ifresid.txt\"\n",
    "\n",
    "# database interaction data\n",
    "file_biogrid_all = \"data/biogrid_all.txt\"\n",
    "file_biogrid_mv_physical = \"data/biogrid_mv_physical.txt\"\n",
    "file_string_all = \"data/string_all.txt\"\n",
    "file_string_physical = \"data/string_physical.txt\"\n",
    "\n",
    "# directory with fasta files (for UniProt accessions)\n",
    "directory_fasta = \"data/gene_sequences/\"\n",
    "\n",
    "# gene names / translations\n",
    "file_translation_hgnc_update = \"data/translation_hgnc_update.txt\"\n",
    "file_translation_hgnc_uniprot = \"data/translation_hgnc_uniprot.txt\"\n",
    "file_translation_hgnc_ensembl = \"data/translation_hgnc_ensembl.txt\"\n",
    "\n",
    "# variant effect predictions\n",
    "file_dat_vep = \"data/cptac_missense_mutations_merge_23.11.1.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030fc924",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ccf1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables for gene name updates / translations\n",
    "translation_hgnc_update = pd.read_csv(file_translation_hgnc_update, sep=\"\\t\")\n",
    "translation_hgnc_uniprot = pd.read_csv(file_translation_hgnc_uniprot, sep=\"\\t\")\n",
    "translation_hgnc_ensembl = pd.read_csv(file_translation_hgnc_ensembl, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51ba716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to subset omics dataframes based on minimum percentage of non-NA values\n",
    "def subset_dataframe(df, percent):\n",
    "    # calculate the percentage of non-NaN values for each gene\n",
    "    gene_counts = df.count(axis=1)\n",
    "    gene_percents = gene_counts / df.shape[1] * 100\n",
    "    # filter based on percent, subset\n",
    "    valid_genes = gene_percents[gene_percents >= percent].index.tolist()\n",
    "    df_subset = df.loc[valid_genes]\n",
    "    \n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70da239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate protein residuals (regressing out RNA) with batch as covariate\n",
    "def calculate_protein_residuals(df):\n",
    "    # convert 'batch' into dummy variables\n",
    "    batch_dummies = pd.get_dummies(df['batch'], drop_first=True).astype(float)\n",
    "    X = pd.concat([df['value_tx'], batch_dummies], axis=1)\n",
    "    model = sm.OLS(df['value_px'], sm.add_constant(X))\n",
    "    result = model.fit()\n",
    "    return result.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f60fc83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate correlations between omic types\n",
    "def calculate_correlation(df, omics1, omics2):\n",
    "    # Group by 'gene' and calculate correlation for each group\n",
    "    def group_corr(g):\n",
    "        return g[omics1].corr(g[omics2])\n",
    "\n",
    "    correlations = df.groupby('gene').apply(group_corr)\n",
    "    col_name = f'corr_{omics1}_{omics2}'\n",
    "    df_return = df.copy()\n",
    "    df_return[col_name] = df_return['gene'].map(correlations)\n",
    "    return df_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb7ab68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def omics_pivot_long(df, omics):\n",
    "    df.reset_index(inplace=True)\n",
    "    df_long = pd.melt(df, id_vars='gene')\n",
    "    df_long.rename(columns={'variable':'sample', 'value':f'value_{omics}'}, inplace=True)\n",
    "    return df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e6cf045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to update list of HGNC gene names \"approved\"\n",
    "def update_hgnc(gene_list):\n",
    "    \n",
    "    # get approved symbols, create dictrionaries\n",
    "    approved_symbols = set(translation_hgnc_update['Approved symbol'])\n",
    "    alias_map = pd.Series(translation_hgnc_update['Approved symbol'].values,\n",
    "                          index=translation_hgnc_update['Alias symbol']).to_dict()\n",
    "    previous_map = pd.Series(translation_hgnc_update['Approved symbol'].values,\n",
    "                             index=translation_hgnc_update['Previous symbol']).to_dict()\n",
    "\n",
    "    # combine dictionaries (if conflict, alias_map will overwrite previous_map), find approved symbol\n",
    "    gene_map = {**previous_map, **alias_map}\n",
    "    translated_genes = [(gene, gene_map.get(gene, gene)) for gene in gene_list]\n",
    "    \n",
    "    # create DataFrame with old and updated names, set gene_hgnc to 'NA' if not in approved symbols\n",
    "    translation_table = pd.DataFrame(translated_genes, columns=['gene_hgnc_data', 'gene_hgnc_approved'])\n",
    "    translation_table['gene_hgnc_approved'] = translation_table['gene_hgnc_approved'].apply(\n",
    "        lambda x: x if x in approved_symbols else np.nan)\n",
    "\n",
    "    return translation_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cac5832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to update HGNC gene names in dataframe to approved and average multi-mappings\n",
    "def update_hgnc_and_average_values(df):\n",
    "    # call update_hgnc to update gene names\n",
    "    gene_list = df.index.tolist()  \n",
    "    translation_table = update_hgnc(gene_list)\n",
    "    df_intermediate = df.reset_index().rename(columns={'gene':'gene_hgnc_data'})\\\n",
    "                                        .merge(translation_table, on='gene_hgnc_data')\\\n",
    "                                        .dropna(subset='gene_hgnc_approved').drop('gene_hgnc_data', axis=1)\n",
    "    df_return = df_intermediate.rename(columns={'gene_hgnc_approved':'gene'}).set_index('gene')\n",
    "    \n",
    "    # new mappings cause duplicates -> identify and average\n",
    "    counts = df_intermediate['gene_hgnc_approved'].value_counts()\n",
    "    multi_entry_genes = counts[counts > 1].index\n",
    "    df_multi = df_intermediate[df_intermediate['gene_hgnc_approved'].isin(multi_entry_genes)]\n",
    "    df_multi_avg = df_multi.groupby('gene_hgnc_approved').mean().reset_index()\n",
    "\n",
    "    # concatenate the single entry genes with the averaged data for multi-entry genes\n",
    "    single_entry_genes = counts[counts == 1].index\n",
    "    df_single = df_intermediate[df_intermediate['gene_hgnc_approved'].isin(single_entry_genes)]\n",
    "    df_return = pd.concat([df_single, df_multi_avg], \n",
    "                        ignore_index=True).rename(columns={'gene_hgnc_approved':'gene'}).set_index('gene')\n",
    "    return df_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e3ce332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to translate between STRING ensembl IDs and HGNC\n",
    "def translate_string(df, translation_table):\n",
    "    # Remove 9606. prefix\n",
    "    df[\"protein1\"] = df[\"protein1\"].str.replace(\"9606.\", \"\", regex=False)\n",
    "    df[\"protein2\"] = df[\"protein2\"].str.replace(\"9606.\", \"\", regex=False)\n",
    "    # Merge on 'protein1'\n",
    "    df_tl = pd.merge(df, translation_table, left_on='protein1', right_on='Protein stable ID', how='left')\n",
    "    df_tl.rename(columns={'Gene name': 'interactor_A'}, inplace=True)\n",
    "    df_tl.drop(['Protein stable ID'], axis=1, inplace=True)\n",
    "\n",
    "    # Merge on 'protein2'\n",
    "    df = pd.merge(df_tl, translation_table, left_on='protein2', right_on='Protein stable ID', how='left')\n",
    "    df.rename(columns={'Gene name': 'interactor_B'}, inplace=True)\n",
    "    df.drop(['Protein stable ID'], axis=1, inplace=True)\n",
    "    \n",
    "    return df[['interactor_A', 'interactor_B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cde3e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function based on update_hgnc() adapted for BioGRID\n",
    "def update_hgnc_biogrid(df, column_name):\n",
    "    # Create intermediate df with translations\n",
    "    df_intermediate = update_hgnc(df[column_name].tolist()).drop_duplicates()\n",
    "    # Merge with updated HGNC names\n",
    "    df = df.merge(df_intermediate, \n",
    "                  left_on=column_name, right_on='gene_hgnc_data', how='left')\n",
    "    \n",
    "    # Drop original column and 'gene_hgnc_data'\n",
    "    df.drop(columns=[column_name, 'gene_hgnc_data'], inplace=True)\n",
    "    \n",
    "    # Rename the 'gene_hgnc_approved' column to original column name\n",
    "    df.rename(columns={'gene_hgnc_approved': column_name}, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "700a5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter & rename biogrid data\n",
    "def filter_biogrid(df):\n",
    "    columns_keep = ['Official Symbol Interactor A', 'Official Symbol Interactor B']\n",
    "    mask = (\n",
    "        (df['Organism Name Interactor A'] == 'Homo sapiens') &\n",
    "        (df['Organism Name Interactor B'] == 'Homo sapiens') &\n",
    "        (df['Experimental System'] != 'Affinity Capture-RNA') &\n",
    "        (df['Experimental System'] != 'Protein-RNA') &\n",
    "        (df['Official Symbol Interactor A'] != df['Official Symbol Interactor B'])\n",
    "    )\n",
    "    \n",
    "    return df[mask][columns_keep].drop_duplicates().rename(columns={'Official Symbol Interactor A':'interactor_A',\n",
    "                                                                    'Official Symbol Interactor B':'interactor_B'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "120a680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that makes df with two interactor columns (_A, _B) redundant\n",
    "def make_redundant(df):\n",
    "    df_reversed = df.rename(columns={\"interactor_A\": \"interactor_B\", \"interactor_B\": \"interactor_A\"})\n",
    "    df_return = pd.concat([df, df_reversed], ignore_index=True).drop_duplicates()\n",
    "    \n",
    "    # remove homodimers\n",
    "    df_return = df_return[df_return['interactor_A']!=df_return['interactor_B']]\n",
    "    return df_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80ee687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval of full sequence and interface residues from FASTA files (via UniProt accession)\n",
    "def get_residues_from_fasta(protein_id, residues_list):\n",
    "    fasta_file = f'{directory_fasta}{protein_id}.fasta'\n",
    "    \n",
    "    # check if fasta file exists\n",
    "    if not os.path.exists(fasta_file):\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    with open(fasta_file, 'r') as fasta_handle:\n",
    "        for record in SeqIO.parse(fasta_handle, 'fasta'):\n",
    "            protein_sequence = str(record.seq)\n",
    "            \n",
    "            # return amino acids if index is within the sequence length, else return np.nan\n",
    "            try:\n",
    "                return ''.join([protein_sequence[i-1] for i in residues_list]), protein_sequence\n",
    "            except IndexError:\n",
    "                return np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "096bc2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that fits two nested linear models and evaluates goodness of fit improvement using LRT\n",
    "# batch is handled as a covariate\n",
    "def nested_models_lrt(df_omics, df_ixn):\n",
    "    # preprocess df_omics for faster access\n",
    "    df_omics_values = df_omics.set_index(['gene', 'sample'])\n",
    "    \n",
    "    # find all interactors B of interactor A based on ixn file, save to dictionary\n",
    "    interactors_dict = defaultdict(list)\n",
    "    for _, row in df_ixn.iterrows():\n",
    "        interactors_dict[row['interactor_A']].append(row['interactor_B'])\n",
    "        \n",
    "    # utils\n",
    "    start_time = time.time()    \n",
    "    results = []\n",
    "    i = 1\n",
    "    tot_ixn = len(interactors_dict)\n",
    "    \n",
    "    # iterate through all interactors A\n",
    "    for gene_A, interactors in interactors_dict.items():\n",
    "        df_omics_A = df_omics_values.loc[gene_A]\n",
    "        print(f'Progress: {i}/{tot_ixn}', end='\\r', flush=True)\n",
    "        i = i+1\n",
    "        \n",
    "        # iterate through all interactors B of interactor A\n",
    "        for gene_B in interactors:\n",
    "            df_omics_B = df_omics_values.loc[gene_B]\n",
    "            merged = df_omics_A.reset_index().merge(df_omics_B.reset_index(), \n",
    "                                                    on=['sample', 'batch'], suffixes=('_A', '_B'))\n",
    "            merged['gene_A'] = gene_A\n",
    "            merged['gene_B'] = gene_B\n",
    "            \n",
    "            # null Model\n",
    "            X_null = sm.add_constant(\n",
    "                pd.concat([merged['value_tx_A'], pd.get_dummies(\n",
    "                    merged['batch'],drop_first=True).astype(float)], axis=1))\n",
    "            \n",
    "            if merged['value_px_A'].empty or X_null.empty:\n",
    "                continue\n",
    "            model_null = sm.OLS(merged['value_px_A'], X_null)\n",
    "            results_null = model_null.fit()\n",
    "\n",
    "            # alternative Model\n",
    "            X_alt = sm.add_constant(pd.concat([merged[['value_tx_A', 'value_cnv_B']], \n",
    "                                               pd.get_dummies(merged['batch'], drop_first=True).astype(float)], axis=1))\n",
    "            model_alt = sm.OLS(merged['value_px_A'], X_alt)\n",
    "            results_alt = model_alt.fit()\n",
    "            \n",
    "            # likelihood Ratio Test\n",
    "            LRT = 2 * (results_alt.llf - results_null.llf)\n",
    "            p_LRT = stats.chi2.sf(LRT, df=1)  # sf is survival function, which is 1 - cdf\n",
    "            \n",
    "            # save regression parameters of alternative model and p-values\n",
    "            params = results_alt.params.values\n",
    "            merged['param_const'] = params[0] # The first parameter is the intercept (const)\n",
    "            merged['param_value_tx_A'] = params[1] # The second parameter is for value_tx_A\n",
    "            merged['param_value_cnv_B'] = params[2] # The third parameter is for value_cnv_B\n",
    "            merged['p_LRT'] = p_LRT\n",
    "            \n",
    "            # drop unused columns, save result\n",
    "            cols_use = ['gene_A', 'gene_B', \n",
    "                        'param_value_cnv_B', 'p_LRT']\n",
    "            merged = merged[cols_use].drop_duplicates()\n",
    "            results.append(merged)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nThe loop took {end_time - start_time} seconds to complete.\")\n",
    "\n",
    "    merged_data = pd.concat(results, ignore_index=True)\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd89eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get current date and time\n",
    "def get_current_datetime_string():\n",
    "    return datetime.now().strftime(\"data_processing_%y%m%d_%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04a2819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map three-letter code to one-letter code\n",
    "def map_to_one_letter(code):\n",
    "    aa_dict = {\n",
    "    'Ala': 'A', 'Arg': 'R', 'Asn': 'N', 'Asp': 'D', 'Cys': 'C', 'Glu': 'E', 'Gln': 'Q', 'Gly': 'G', 'His': 'H',\n",
    "    'Ile': 'I', 'Leu': 'L', 'Lys': 'K', 'Met': 'M', 'Phe': 'F', 'Pro': 'P', 'Ser': 'S', 'Thr': 'T', 'Trp': 'W',\n",
    "    'Tyr': 'Y', 'Val': 'V'\n",
    "    }\n",
    "    if isinstance(code, str):\n",
    "        if code in aa_dict:\n",
    "            return aa_dict[code]\n",
    "        elif len(code) == 1:\n",
    "            return code\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "163531b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to categorize FoldX score\n",
    "def categorize_foldx(score):\n",
    "    if pd.isna(score):\n",
    "        return np.nan\n",
    "    elif score > foldx_threshold_destabilizing:\n",
    "        return 'destabilizing'\n",
    "    elif score < foldx_threshold_stabilizing:\n",
    "        return 'stabilizing'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6433abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to categorize EVE score\n",
    "def categorize_eve(score):\n",
    "    if pd.isna(score):\n",
    "        return np.nan\n",
    "    elif score > eve_threshold:\n",
    "        return 'pathogenic'\n",
    "    else:\n",
    "        return 'benign'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1af63",
   "metadata": {},
   "source": [
    "# Prepare CPTAC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa208f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CPTAC data\n",
    "dat_annot = pd.read_csv(file_dat_annot, sep=\"\\t\")\n",
    "dat_cnv = pd.read_csv(file_dat_cnv, sep=\"\\t\")\n",
    "dat_tx = pd.read_csv(file_dat_tx, sep=\"\\t\")\n",
    "dat_px = pd.read_csv(file_dat_px, sep=\"\\t\")\n",
    "\n",
    "dat_cnv.set_index('gene', inplace=True)\n",
    "dat_tx.set_index('gene', inplace=True)\n",
    "dat_px.set_index('gene', inplace=True)\n",
    "\n",
    "# SUBSET SAMPLES\n",
    "# define batches to exclude from analysis\n",
    "\n",
    "#-------------------------------------------------------- PARAMETER --------------------------------------------------------#\n",
    "exclude_batches = ['ccle', 'cbttc', 'tcga-coread']\n",
    "#---------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "samples_tissue = set(dat_annot[~dat_annot['batch'].isin(exclude_batches)]['sample'])\n",
    "\n",
    "# find common samples, subset\n",
    "common_samples = list(set(dat_cnv.columns) & set(dat_tx.columns) & \n",
    "                      set(dat_px.columns) & samples_tissue)\n",
    "dat_cnv = dat_cnv[common_samples]\n",
    "dat_tx = dat_tx[common_samples]\n",
    "dat_px = dat_px[common_samples]\n",
    "\n",
    "# SUBSET GENES\n",
    "# throw away genes with low coverage and low RNA-protein correlation\n",
    "\n",
    "#-------------------------------------------------------- PARAMETER --------------------------------------------------------#\n",
    "min_percent = 25\n",
    "#---------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "dat_cnv = subset_dataframe(dat_cnv, percent=min_percent)\n",
    "dat_tx = subset_dataframe(dat_tx, percent=min_percent)\n",
    "dat_px = subset_dataframe(dat_px, percent=min_percent)\n",
    "\n",
    "# find common genes, subset\n",
    "common_genes = list(set(dat_tx.index) & set(dat_px.index) & set(dat_cnv.index))\n",
    "dat_cnv = dat_cnv.loc[common_genes]\n",
    "dat_tx = dat_tx.loc[common_genes]\n",
    "dat_px = dat_px.loc[common_genes]\n",
    "\n",
    "# UPDATE HGNC SYMBOLS\n",
    "dat_cnv = update_hgnc_and_average_values(dat_cnv)\n",
    "dat_tx = update_hgnc_and_average_values(dat_tx)\n",
    "dat_px = update_hgnc_and_average_values(dat_px)\n",
    "\n",
    "batch_counts = dat_annot[['sample', 'batch']][dat_annot['sample'].isin(common_samples)].drop_duplicates().batch.value_counts()\n",
    "df_batch_counts = pd.DataFrame(batch_counts).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7de9ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot omics measurement dfs to long, merge\n",
    "dat_cnv_long = omics_pivot_long(dat_cnv, omics='cnv')\n",
    "dat_tx_long = omics_pivot_long(dat_tx, omics='tx')\n",
    "dat_px_long = omics_pivot_long(dat_px, omics='px')\n",
    "dat_omics_int = dat_cnv_long.merge(dat_tx_long, on=['gene', 'sample'], how='inner')\n",
    "dat_omics = dat_omics_int.merge(dat_px_long, on=['gene', 'sample'], how='inner')\n",
    "\n",
    "# only keep samples with cnv, tx and px measurements\n",
    "dat_omics.dropna(inplace=True)\n",
    "\n",
    "# annotate, rearrange\n",
    "dat_annot = dat_annot[['sample', 'batch']].drop_duplicates()\n",
    "dat_omics = dat_omics.merge(dat_annot, on='sample', how='left')\n",
    "\n",
    "order_cols = ['gene', 'sample', 'batch', 'value_cnv', 'value_tx', 'value_px']\n",
    "dat_omics = dat_omics[order_cols]\n",
    "dat_omics = dat_omics.sort_values(by=['batch', 'sample', 'gene'])\n",
    "dat_omics.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# for each gene, calculate protein residuals\n",
    "residuals = dat_omics.groupby('gene').apply(calculate_protein_residuals).reset_index(level=0, drop=True)\n",
    "dat_omics['residuals'] = residuals\n",
    "\n",
    "# for each gene, calculate correlation between RNA and protein\n",
    "dat_omics = calculate_correlation(dat_omics, 'value_tx', 'value_px')\n",
    "\n",
    "# store subsetted & updated gene names for later use\n",
    "common_genes = dat_omics['gene'].unique().tolist()\n",
    "common_samples = dat_omics['sample'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "642c6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_mt = pd.read_csv(file_dat_mt, sep=\"\\t\")\n",
    "\n",
    "dat_mt.rename(columns={'gene_symbol':'gene'}, inplace=True)\n",
    "\n",
    "dat_mt = dat_mt[(dat_mt['gene'].isin(common_genes)) & \n",
    "                (dat_mt['sample'].isin(common_samples))]\n",
    "\n",
    "# split the \"HGVSp\" column into three new columns, standardize to one letter code\n",
    "dat_mt[['pREF', 'pPOS', 'pALT']] = dat_mt['HGVSp'].str.extract(r'p\\.([A-Za-z]+)(\\d+)([A-Za-z]+)')\n",
    "dat_mt['pREF'] = dat_mt['pREF'].str.capitalize().apply(map_to_one_letter)\n",
    "dat_mt['pALT'] = dat_mt['pALT'].str.capitalize().apply(map_to_one_letter)\n",
    "\n",
    "dat_mt['pREF'] = dat_mt['pREF'].astype(str)\n",
    "dat_mt['pPOS'] = dat_mt['pPOS'].astype(str)\n",
    "dat_mt['pALT'] = dat_mt['pALT'].astype(str)\n",
    "\n",
    "dat_mt_var_cl = dat_mt[['gene', 'sample', 'variant_class', 'pPOS', 'pREF', 'pALT']].drop_duplicates()\n",
    "\n",
    "variant_classes = ['Missense_Mutation', 'Frame_Shift_Del', 'Nonsense_Mutation']\n",
    "\n",
    "dat_mt_var_cl = dat_mt_var_cl[dat_mt_var_cl['variant_class'].isin(variant_classes)]\n",
    "\n",
    "# If there is a nonsense mutation in a gene, it will have a higher impact \n",
    "# on protein abundance/functionality than a co-occurring missense mutation. \n",
    "# In these cases, the gene will be regarded as only nonsense-mutated\n",
    "\n",
    "# Define the priority for each variant class\n",
    "priority_map = {\n",
    "    'Nonsense_Mutation': 1,\n",
    "    'Frame_Shift_Del': 2,\n",
    "    'Missense_Mutation': 3\n",
    "}\n",
    "\n",
    "# Function to get the highest priority variant\n",
    "def get_highest_priority_variant(group):\n",
    "    return group.loc[group['variant_class'].map(priority_map).idxmin()]\n",
    "\n",
    "# Sort by gene, sample, and priority\n",
    "dat_mt_sort = dat_mt_var_cl.sort_values(by=['gene', 'sample', 'variant_class'], key=lambda x: x.map(priority_map))\n",
    "\n",
    "# Group by gene and sample, and apply the function to get the highest priority variant\n",
    "dat_mt_prio = dat_mt_sort.groupby(['gene', 'sample']).apply(get_highest_priority_variant).reset_index(drop=True)\n",
    "dat_omics_mt_prio = dat_omics.merge(dat_mt_prio, on=['gene', 'sample'], how='inner')\n",
    "\n",
    "some_mutation = dat_mt[['sample', 'gene']].drop_duplicates()\n",
    "\n",
    "# Merge with an indicator to find matching rows\n",
    "dat_omics_no_mt = dat_omics.merge(some_mutation, on=['sample', 'gene'], how='left', indicator=True)\n",
    "\n",
    "# Filter out rows that are found in both dataframes\n",
    "dat_omics_no_mt = dat_omics_no_mt[dat_omics_no_mt['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "dat_omics_no_mt['variant_class'] = 'No_Mutation'\n",
    "\n",
    "dat_omics_mt = pd.concat([dat_omics_mt_prio, dat_omics_no_mt], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6ca53f",
   "metadata": {},
   "source": [
    "### Remove hypermutated samples, genes with low RNA-Protein correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97052dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discard genes with low/negative mRNA-protein correlation (can also be further subsetted in data_viz.ipynb)\n",
    "#-------------------------------------------------------- PARAMETER --------------------------------------------------------#\n",
    "correlation_cutoff = 0\n",
    "#---------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# subset according to correlation cutoff\n",
    "dat_omics_mt = dat_omics_mt[dat_omics_mt['corr_value_tx_value_px']>correlation_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85d426f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------- PARAMETER --------------------------------------------------------#\n",
    "remove_hypermutated = True\n",
    "#---------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "mutations_per_sample = dat_omics_mt[dat_omics_mt['variant_class']!='No_Mutation'].groupby('sample').size().reset_index(name='mutations_count')\n",
    "\n",
    "# Remove hypermutated samples\n",
    "hypermutated_samples = mutations_per_sample[mutations_per_sample['mutations_count']>500]['sample']\n",
    "if remove_hypermutated:\n",
    "    # remove from mutations dataset\n",
    "    dat_omics_mt = dat_omics_mt[~dat_omics_mt['sample'].isin(hypermutated_samples)]\n",
    "    \n",
    "    # remove from omics dataset\n",
    "    dat_omics = dat_omics[~dat_omics['sample'].isin(hypermutated_samples)]\n",
    "    common_samples = dat_omics['sample'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ff507",
   "metadata": {},
   "source": [
    "# Prepare DB interactions (STRING, BioGRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c00ea0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read STRING data\n",
    "string_all = pd.read_csv(file_string_all, sep=' ')\n",
    "string_physical = pd.read_csv(file_string_physical, sep=' ')\n",
    "\n",
    "# Translate protein IDs to gene names\n",
    "string_all_translated = translate_string(string_all, translation_hgnc_ensembl)\n",
    "string_physical_translated = translate_string(string_physical, translation_hgnc_ensembl)\n",
    "\n",
    "# Drop NAs, duplicates\n",
    "string_all_translated = string_all_translated.dropna(axis=0).drop_duplicates()\n",
    "string_physical_translated = string_physical_translated.dropna(axis=0).drop_duplicates()\n",
    "\n",
    "# Make both redundant\n",
    "string_all_redundant = make_redundant(string_all_translated)\n",
    "string_physical_redundant = make_redundant(string_physical_translated)\n",
    "\n",
    "# Outer merge with set definition\n",
    "string = pd.merge(string_all_redundant, string_physical_redundant, \n",
    "                  on=['interactor_A', 'interactor_B'], how='outer', indicator=True)\n",
    "\n",
    "string.rename(columns={'_merge':'string_set'}, inplace=True)\n",
    "string['string_set'] = string['string_set'].map({'both': 'physical', 'left_only': 'general'})\n",
    "\n",
    "# Filter based on common genes\n",
    "string = string[(string['interactor_A'].isin(common_genes)) & \n",
    "                (string['interactor_B'].isin(common_genes))].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d93e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read BioGRID data\n",
    "biogrid_all = pd.read_csv(file_biogrid_all, sep='\\t', low_memory=False)\n",
    "biogrid_mv_physical = pd.read_csv(file_biogrid_mv_physical, sep='\\t', low_memory=False)\n",
    "\n",
    "# Filter using the new function\n",
    "biogrid_all = filter_biogrid(biogrid_all)\n",
    "biogrid_physical = filter_biogrid(biogrid_mv_physical)\n",
    "\n",
    "# Update interactor_A and interactor_B names for biogrid_all\n",
    "biogrid_all = update_hgnc_biogrid(biogrid_all, 'interactor_A')\n",
    "biogrid_all = update_hgnc_biogrid(biogrid_all, 'interactor_B')\n",
    "\n",
    "# Update interactor_A and interactor_B names for biogrid_physical\n",
    "biogrid_physical = update_hgnc_biogrid(biogrid_physical, 'interactor_A')\n",
    "biogrid_physical = update_hgnc_biogrid(biogrid_physical, 'interactor_B')\n",
    "\n",
    "# Drop NAs, duplicates\n",
    "biogrid_all = biogrid_all.dropna(axis=0).drop_duplicates()\n",
    "biogrid_physical = biogrid_physical.dropna(axis=0).drop_duplicates()\n",
    "\n",
    "# Make both redundant\n",
    "biogrid_all = make_redundant(biogrid_all)\n",
    "biogrid_physical = make_redundant(biogrid_physical)\n",
    "\n",
    "# Outer merge with set definition\n",
    "biogrid = pd.merge(biogrid_all, biogrid_physical, \n",
    "                   on=['interactor_A', 'interactor_B'], how='outer', indicator=True)\n",
    "\n",
    "biogrid.rename(columns={'_merge':'biogrid_set'}, inplace=True)\n",
    "biogrid['biogrid_set'] = biogrid['biogrid_set'].map({'both': 'physical', 'left_only': 'general'})\n",
    "\n",
    "# Filter based on common genes\n",
    "biogrid = biogrid[(biogrid['interactor_A'].isin(common_genes)) & \n",
    "                  (biogrid['interactor_B'].isin(common_genes))].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e8b9f1",
   "metadata": {},
   "source": [
    "# Prepare interaction data (David)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "817a1435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read interaction data\n",
    "dat_res = pd.read_csv(file_dat_res, sep='\\t')\n",
    "dat_res\n",
    "\n",
    "# only consider moderate-to-high confidence interaction models\n",
    "\n",
    "#-------------------------------------------------------- PARAMETER --------------------------------------------------------#\n",
    "min_pdockq = 0.23\n",
    "#---------------------------------------------------------------------------------------------------------------------------#\n",
    "dat_res = dat_res[dat_res['pdockq'] > min_pdockq]\n",
    "\n",
    "# split the dataframe into two: one for chain A and one for chain B\n",
    "dat_res_A = dat_res[dat_res['chain'] == 'A'].reset_index(drop=True)\n",
    "dat_res_B = dat_res[dat_res['chain'] == 'B'].reset_index(drop=True)\n",
    "\n",
    "# rename the columns to distinguish between chain A and chain B\n",
    "#dat_res_A = dat_res_A.rename(columns={'protein': 'protein1', 'residues': 'residues1'})\n",
    "#dat_res_B = dat_res_B.rename(columns={'protein': 'protein2', 'residues': 'residues2'})\n",
    "dat_res_A = dat_res_A.rename(columns={'protein': 'protein1', 'resseq': 'residues1'})\n",
    "dat_res_B = dat_res_B.rename(columns={'protein': 'protein2', 'resseq': 'residues2'})\n",
    "dat_res_B = dat_res_B[['protein2', 'residues2']]\n",
    "dat_res = pd.concat([dat_res_A, dat_res_B], axis=1)\n",
    "\n",
    "# drop unnecessary columns, rename\n",
    "#dat_res = dat_res.drop(columns=['chain', 'clashes'])\n",
    "dat_res = dat_res.drop(columns=['chain'])\n",
    "dat_res = dat_res[['pair', 'pdockq', 'protein1', 'residues1', 'protein2', 'residues2']]\n",
    "col_names = ['pair', 'pDockQ', 'protein1', 'residues1', 'protein2', 'residues2']\n",
    "dat_res.columns = col_names\n",
    "\n",
    "dat_res.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b0ad6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing protein1: 100%|███████████████████████████████████████████████████| 103863/103863 [00:55<00:00, 1882.34it/s]\n",
      "Processing protein2: 100%|███████████████████████████████████████████████████| 103863/103863 [00:48<00:00, 2145.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# apply the function to each row of the data frame for protein1 and protein2\n",
    "tqdm.pandas(desc=\"Processing protein1\")\n",
    "dat_res[['residues1_aa', 'sequence1']] = dat_res.progress_apply(\n",
    "    lambda row: pd.Series(get_residues_from_fasta(row['protein1'], list(map(int, row['residues1'].split(','))))), axis=1)\n",
    "\n",
    "tqdm.pandas(desc=\"Processing protein2\")\n",
    "dat_res[['residues2_aa', 'sequence2']] = dat_res.progress_apply(\n",
    "    lambda row: pd.Series(get_residues_from_fasta(row['protein2'], list(map(int, row['residues2'].split(','))))), axis=1)\n",
    "\n",
    "dat_res.dropna(axis=0, inplace=True)\n",
    "\n",
    "# count residues\n",
    "residues1_lists = dat_res['residues1'].apply(lambda x: list(map(int, x.split(','))))\n",
    "residues2_lists = dat_res['residues2'].apply(lambda x: list(map(int, x.split(','))))\n",
    "dat_res['NumRes1'] = residues1_lists.apply(len)\n",
    "dat_res['NumRes2'] = residues2_lists.apply(len)\n",
    "dat_res['NumRes'] = dat_res['NumRes1'] + dat_res['NumRes2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2507ad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe redundant (A->B, B->A)\n",
    "dat_res_copy = dat_res.copy()\n",
    "\n",
    "# switch necessary columns in the copied dataframe\n",
    "dat_res_copy[['protein1', 'protein2']] = dat_res_copy[['protein2', 'protein1']]\n",
    "dat_res_copy[['residues1', 'residues2']] = dat_res_copy[['residues2', 'residues1']]\n",
    "dat_res_copy[['residues1_aa', 'residues2_aa']] = dat_res_copy[['residues2_aa', 'residues1_aa']]\n",
    "dat_res_copy[['NumRes1', 'NumRes2']] = dat_res_copy[['NumRes2', 'NumRes1']]\n",
    "dat_res_copy[['sequence1', 'sequence2']] = dat_res_copy[['sequence2', 'sequence1']]\n",
    "\n",
    "# append the copied dataframe to the original dataframe\n",
    "dat_res = pd.concat([dat_res, dat_res_copy], ignore_index=True)\n",
    "dat_res.drop_duplicates(inplace=True)\n",
    "\n",
    "# rename columns\n",
    "dat_res.rename(columns={'protein1':'interactor_A', 'protein2':'interactor_B', \n",
    "                        'residues1':'index_residues_A', 'residues2':'index_residues_B', \n",
    "                        'NumRes1':'NumRes_A', 'NumRes2':'NumRes_B', \n",
    "                        'residues1_aa':'residues_A', 'residues2_aa':'residues_B', \n",
    "                        'sequence1':'sequence_A', 'sequence2':'sequence_B'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97b6a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare translation table\n",
    "translation_hgnc_uniprot = translation_hgnc_uniprot[['Approved symbol', 'UniProt accession']].drop_duplicates().dropna(axis=0)\n",
    "# only keep HGNC names that appear in data (avoids surjective mappings)\n",
    "translation_hgnc_uniprot = translation_hgnc_uniprot[translation_hgnc_uniprot['Approved symbol'].isin(common_genes)]\n",
    "# throw away injective mappings (very few cases)\n",
    "translation_hgnc_uniprot = translation_hgnc_uniprot.drop_duplicates(subset='UniProt accession', keep='first')\n",
    "\n",
    "# translate (tl) interaction data\n",
    "dat_res_tl = dat_res.merge(translation_hgnc_uniprot, left_on='interactor_A', right_on='UniProt accession', how='left')\n",
    "dat_res_tl = dat_res_tl.drop(columns=['UniProt accession', 'interactor_A'])\n",
    "dat_res_tl = dat_res_tl.rename(columns={'Approved symbol': 'interactor_A'})\n",
    "dat_res_tl = dat_res_tl.merge(translation_hgnc_uniprot, left_on='interactor_B', right_on='UniProt accession', how='left')\n",
    "dat_res_tl = dat_res_tl.drop(columns=['UniProt accession', 'interactor_B'])\n",
    "dat_res_tl = dat_res_tl.rename(columns={'Approved symbol': 'interactor_B'})\n",
    "dat_res_tl.dropna(axis=0, inplace=True)\n",
    "\n",
    "# drop Homodimers\n",
    "dat_res_tl = dat_res_tl[dat_res_tl['interactor_A']!=dat_res_tl['interactor_B']]\n",
    "\n",
    "# translation causes multi-mappings -> average\n",
    "numeric_cols = ['pDockQ', 'NumRes_A', 'NumRes_B', 'NumRes']\n",
    "dat_res_numeric = dat_res_tl.groupby(['interactor_A', 'interactor_B'])[numeric_cols].mean().reset_index()\n",
    "dat_res_numeric['NumRes'] = dat_res_numeric['NumRes'].round().astype(int)\n",
    "dat_res_numeric['NumRes_A'] = dat_res_numeric['NumRes_A'].round().astype(int)\n",
    "dat_res_numeric['NumRes_B'] = dat_res_numeric['NumRes_B'].round().astype(int)\n",
    "dat_res_non_numeric = dat_res_tl.groupby(['interactor_A', 'interactor_B'], as_index=False).first()\n",
    "dat_res_non_numeric = dat_res_non_numeric.drop(columns=dat_res_numeric.columns[2:])\n",
    "dat_res_avg = pd.merge(dat_res_numeric, dat_res_non_numeric, on=['interactor_A', 'interactor_B'])\n",
    "\n",
    "# order, drop columns\n",
    "cols_use = ['interactor_A', 'interactor_B', 'NumRes_A', 'NumRes_B', 'NumRes', 'pDockQ', \n",
    "            'residues_A', 'residues_B', 'index_residues_A', 'index_residues_B', 'sequence_A', 'sequence_B']\n",
    "\n",
    "dat_ixn = dat_res_avg[cols_use]\n",
    "dat_ixn.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "874510ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile database info\n",
    "db_info = string.merge(biogrid, on=['interactor_A', 'interactor_B'], how='outer')\n",
    "\n",
    "# merge with database information\n",
    "dat_ixn = dat_ixn.merge(string, on=['interactor_A', 'interactor_B'], how='left')\n",
    "dat_ixn['string_set'] = dat_ixn['string_set'].fillna('None')\n",
    "\n",
    "dat_ixn = dat_ixn.merge(biogrid, on=['interactor_A', 'interactor_B'], how='left')\n",
    "dat_ixn['biogrid_set'] = dat_ixn['biogrid_set'].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2b2be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_physical = db_info[(db_info['string_set']=='physical') | (db_info['biogrid_set']=='physical')]\n",
    "no_db_physical = db_physical.shape[0]/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "653343a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider interactions with additional database evidence\n",
    "\n",
    "#-------------------------------------------------------- PARAMETER --------------------------------------------------------#\n",
    "string_sets = ['physical']\n",
    "biogrid_sets = ['physical']\n",
    "#---------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Define logic as lambda functions\n",
    "AND_logic = lambda x, y: x & y\n",
    "OR_logic = lambda x, y: x | y\n",
    "\n",
    "# Determine which logic to use\n",
    "\n",
    "#-------------------------------------------------------- PARAMETER --------------------------------------------------------#\n",
    "logic = OR_logic\n",
    "#---------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Save logic as string for parameter export\n",
    "if logic == AND_logic:\n",
    "    logic_str = \"AND\"\n",
    "else:\n",
    "    logic_str = \"OR\"\n",
    "\n",
    "# subset dat_ixn\n",
    "dat_ixn = dat_ixn[logic(dat_ixn['string_set'].isin(string_sets), dat_ixn['biogrid_set'].isin(biogrid_sets))]\n",
    "\n",
    "# find no of unique interactions\n",
    "unique_ixn = dat_ixn.shape[0]/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd51eac",
   "metadata": {},
   "source": [
    "# Nested linear models, LRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eabf6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCREASE MEMORY EFFICIENCY\n",
    "# convert float64 to float32\n",
    "float64_cols = dat_omics.select_dtypes(include='float64').columns\n",
    "dat_omics[float64_cols] = dat_omics[float64_cols].astype('float32')\n",
    "\n",
    "# convert batch to category\n",
    "dat_omics['batch'] = dat_omics['batch'].astype('category')\n",
    "\n",
    "# drop unused columns\n",
    "columns_use = ['gene', 'sample', 'batch', 'value_cnv', 'value_tx', 'value_px', 'residuals', 'corr_value_tx_value_px']\n",
    "dat_omics = dat_omics[columns_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23eb2fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 7170/7170\n",
      "The loop took 1151.940081834793 seconds to complete.\n"
     ]
    }
   ],
   "source": [
    "dat_lrt = nested_models_lrt(df_omics=dat_omics, df_ixn=dat_ixn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5add57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_lrt = dat_lrt.merge(dat_ixn, \n",
    "                        left_on=['gene_A', 'gene_B'], \n",
    "                        right_on=['interactor_A', 'interactor_B'], \n",
    "                        how='inner').drop(['interactor_A', 'interactor_B'], axis=1)\n",
    "\n",
    "# adjust p-values for false discovery rate using the Benjamini-Hochberg method\n",
    "dat_lrt['p_LRT_adj'] = multipletests(dat_lrt['p_LRT'].dropna().values, method='fdr_bh')[1]\n",
    "dat_lrt['neg_log10_p_LRT_adj'] = -np.log10(dat_lrt['p_LRT_adj'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78814668",
   "metadata": {},
   "source": [
    "# Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f70289f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds for classifying into deleterious/non-deleterious\n",
    "#-------------------------------------------------------- PARAMETER --------------------------------------------------------#\n",
    "foldx_threshold_destabilizing = 2\n",
    "foldx_threshold_stabilizing = -2\n",
    "eve_threshold = 0.7\n",
    "#---------------------------------------------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb9713e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read VEP file\n",
    "dat_vep = pd.read_csv(file_dat_vep, sep=\"\\t\")\n",
    "\n",
    "# subset & rename columns\n",
    "keep_columns = ['sample', 'gene', 'pPOS', 'pREF', 'pALT', \n",
    "                'ESM1b_LLR', 'ESM1b_is_pathogenic', 'am_pathogenicity', 'am_class', 'eve', 'pred_ddg', 'plddt']\n",
    "dat_vep = dat_vep[keep_columns]\n",
    "\n",
    "dat_vep.rename(columns={'ESM1b_LLR':'ESM_score', 'ESM1b_is_pathogenic':'ESM_cat', \n",
    "                        'am_pathogenicity':'AM_score', 'am_class':'AM_cat', \n",
    "                        'eve':'EVE_score', 'pred_ddg':'FoldX_score', 'plddt':'pLDDT'}, inplace=True)\n",
    "\n",
    "# subset common samples and common genes\n",
    "dat_vep = dat_vep[(dat_vep['gene'].isin(common_genes)) & (dat_vep['sample'].isin(common_samples))]\n",
    "\n",
    "# -99 is NA\n",
    "dat_vep['EVE_score'] = dat_vep['EVE_score'].replace(-99, np.nan)\n",
    "dat_vep['pLDDT'] = dat_vep['pLDDT'].replace(-99, np.nan)\n",
    "dat_vep['FoldX_score'] = dat_vep['FoldX_score'].replace(-99, np.nan)\n",
    "\n",
    "# drop empty data\n",
    "dat_vep_null = dat_vep[dat_vep[['ESM_score', 'ESM_cat', \n",
    "                                'AM_score', 'AM_cat', \n",
    "                                'EVE_score', 'FoldX_score', 'pLDDT']].isnull().all(axis=1)]\n",
    "dat_vep = dat_vep.drop(dat_vep_null.index)\n",
    "\n",
    "# Apply categorize_foldx to create a new column 'FoldX_cat'\n",
    "dat_vep['FoldX_cat'] = dat_vep['FoldX_score'].apply(categorize_foldx)\n",
    "dat_vep['EVE_cat'] = dat_vep['EVE_score'].apply(categorize_eve)\n",
    "\n",
    "# In some samples, on gene carries multiple mutations at different position -> only keep most deleterious\n",
    "multiple_mutations = dat_vep[dat_vep.duplicated(subset=['gene', 'sample'], keep=False)]\n",
    "multiple_mutations_sorted = multiple_mutations.sort_values(by=['gene', 'sample', 'AM_score'], \n",
    "                                                           ascending=[True, True, False])\n",
    "multiple_mutations_filtered = multiple_mutations_sorted.drop_duplicates(subset=['gene', 'sample'], keep='first')\n",
    "dat_vep = dat_vep.drop(multiple_mutations.index)\n",
    "dat_vep = pd.concat([dat_vep, multiple_mutations_filtered], ignore_index=True)\n",
    "\n",
    "# merge vep and omics data\n",
    "dat_vep_omics = pd.merge(dat_vep, dat_omics, on=['gene', 'sample'], how='left')\n",
    "# drop vep data without omics measurements\n",
    "dat_vep_omics_null = dat_vep_omics[dat_vep_omics[['value_cnv', 'value_tx', 'value_px']].isnull().all(axis=1)]\n",
    "dat_vep_omics = dat_vep_omics.drop(dat_vep_omics_null.index)\n",
    "\n",
    "dat_vep_omics = dat_vep_omics[dat_vep_omics['corr_value_tx_value_px']>correlation_cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5220ef09",
   "metadata": {},
   "source": [
    "# Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "271146a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder name and path using current date and time\n",
    "folder_name = f\"{get_current_datetime_string()}\"\n",
    "full_path = os.path.join(\"data_computed\", folder_name)\n",
    "os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "# save the files\n",
    "dat_omics.to_csv(os.path.join(full_path, \"omics.txt\"), sep=\"\\t\", index=False)\n",
    "dat_omics_mt.to_csv(os.path.join(full_path, \"omics_mt.txt\"), sep=\"\\t\", index=False)\n",
    "dat_lrt.to_csv(os.path.join(full_path, \"lrt.txt\"), sep=\"\\t\", index=False)\n",
    "dat_vep_omics.to_csv(os.path.join(full_path, \"vep_omics.txt\"), sep=\"\\t\", index=False)\n",
    "df_batch_counts.to_csv(os.path.join(full_path, \"batch_counts.txt\"), sep=\"\\t\", index=False)\n",
    "\n",
    "# write parameters used for this analysis to file\n",
    "params_path = os.path.join(full_path, \"parameters.txt\")\n",
    "\n",
    "# Write the parameters to parameters.txt\n",
    "with open(params_path, 'w') as f:\n",
    "    f.write(f\"min_pdockq: {min_pdockq}\\n\")\n",
    "    f.write('\\n')\n",
    "    f.write(f\"Interaction in STRING ({', '.join(string_sets)}) {logic_str} in BioGRID ({', '.join(biogrid_sets)})\\n\")\n",
    "    f.write('\\n')\n",
    "    f.write(f'No. unique interactions analyzed: {unique_ixn}')\n",
    "    f.write('\\n')\n",
    "    f.write(f'Correlation cutoff: {correlation_cutoff}')\n",
    "    f.write('\\n')\n",
    "    if remove_hypermutated:\n",
    "        f.write('Remove hypermutated samples: True')\n",
    "    else: \n",
    "        f.write('Remove hypermutated samples: False')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sp2] *",
   "language": "python",
   "name": "conda-env-sp2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
